>**NOTICE!**
> * Support has moved from `keras` to `tensorflow.keras` framework. 
> * There was a large update on 2020-12-17, see in [Changelog](CHANGELOG.md) what has changed.

# ImageDataAugmentor
`ImageDataAugmentor` is a custom image data generator for `tensorflow.keras` 
that supports `albumentations`.

To learn more about:
* `ImageDataGenerator`, see:
  https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator
* `albumentations`, see:
  https://github.com/albumentations-team/albumentations


### Installation 
* For `tensorflow.keras`:
```shell
$ pip install git+https://github.com/mjkvaak/ImageDataAugmentor
```

* (Deprecated) for `keras` you can still install `ImageDataAugmentor` 
  from the last commit that supported the framework:
```shell
$ pip install git+https://github.com/mjkvaak/ImageDataAugmentor.git@ce9bf4d7d532bfcb14fda7fb43d7bcdc6d7990ff
```

### How to use
The usage is analogous to `tensorflow.keras`' `ImageDataGenerator` with
the exception that the image transformations will be generated using
external augmentations library `albumentations`.
Below are examples of some commonly encountered use cases.

>Pro tip: you can use `.show_data()` to visualize a random bunch
> of images generated by `ImageDataAugmentor`.

**Example of using `.flow_from_directory(directory)` with `albumentations`**:
```python
from ImageDataAugmentor.image_data_augmentor import *
import albumentations

...
    
AUGMENTATIONS = albumentations.Compose([
    albumentations.Transpose(p=0.5),
    albumentations.Flip(p=0.5),
    albumentations.OneOf([
        albumentations.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),
        albumentations.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1)
    ],p=1),
    albumentations.GaussianBlur(p=0.05),
    albumentations.HueSaturationValue(p=0.5),
    albumentations.RGBShift(p=0.5),
])

train_datagen = ImageDataAugmentor(
        rescale=1./255,
        augment=AUGMENTATIONS,
        preprocess_input=None)
train_generator = train_datagen.flow_from_directory(
        'data/train',
        target_size=(224, 224),
        batch_size=32,
        class_mode='binary')
val_datagen = ImageDataAugmentor(rescale=1./255)
validation_generator = val_datagen.flow_from_directory(
        'data/validation',
        target_size=(224, 224),
        batch_size=32,
        class_mode='binary')
#train_generator.show_data() #<- visualize a bunch of augmented data

model.fit(
        train_generator,
        steps_per_epoch=len(train_generator),
        epochs=50,
        validation_data=validation_generator,
        validation_steps=len(validation_generator))
...
```

**Example of using `.flow(x, y)` with `albumentations.imgaug`:**
```python
from ImageDataAugmentor.image_data_augmentor import *
from albumentations.imgaug.transforms import ia, iaa
...

sometimes = lambda aug: iaa.Sometimes(0.5, aug)
AUGMENTATIONS = iaa.Sequential([
    iaa.Fliplr(0.5), # horizontally flip 50% of all images
    iaa.Flipud(0.2), # vertically flip 20% of all images
    sometimes(iaa.Affine(
        scale={"x": (0.9, 1.1), "y": (0.9, 1.1)}, # scale images to 90-110% of their size, individually per axis
        translate_percent={"x": (-0.1, 0.1), "y": (-0.1, 0.1)}, # translate by -10 to +10 percent (per axis)
        rotate=(-45, 45), # rotate by -45 to +45 degrees
        shear=(-5, 5), # shear by -5 to +5 degrees
        mode=ia.ALL # use any of scikit-image's warping modes
    )
    )],
    random_order=True)    

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
y_train = np_utils.to_categorical(y_train, num_classes)
y_test = np_utils.to_categorical(y_test, num_classes)

datagen = ImageDataAugmentor(
    featurewise_center=True,
    featurewise_std_normalization=True,
    augment=AUGMENTATIONS)

# compute quantities required for featurewise normalization
datagen.fit(x_train)

#train_generator.show_data() #<- visualize a bunch of augmented data

# fits the model on batches with real-time data augmentation:
model.fit(datagen.flow(x_train, y_train, batch_size=32),
          steps_per_epoch=len(x_train) / 32, epochs=epochs)
```    

**Example of using `.flow_from_directory()` with masks for segmentation with `albumentations`:**

```python    
from ImageDataAugmentor.image_data_augmentor import *
import albumentations

...
SEED = 123
AUGMENTATIONS = albumentations.Compose([
  albumentations.HorizontalFlip(p=0.5),
  albumentations.ElasticTransform(),
])

# Assume that DATA_DIR has subdirs "images" and "masks", 
# where masks have been saved as grayscale images with pixel value
# denoting the segmentation label
DATA_DIR = ... 
N_CLASSES = ... # number of segmentation classes in masks

def one_hot_encode_masks(y:np.array, classes=range(N_CLASSES)):
    ''' One hot encodes target masks for segmentation '''
    y = y.squeeze()
    masks = [(y == v) for v in classes]
    mask = np.stack(masks, axis=-1).astype('float')
    # add background if the mask is not binary
    if mask.shape[-1] != 1:
        background = 1 - mask.sum(axis=-1, keepdims=True)
        mask = np.concatenate((mask, background), axis=-1)
    return mask

img_data_gen = ImageDataAugmentor(
    augment=AUGMENTATIONS, 
    input_augment_mode='image', 
    validation_split=0.2,
    seed=SEED,
)
mask_data_gen = ImageDataAugmentor(
    augment=AUGMENTATIONS, 
    input_augment_mode='mask', #<- notice the different augment mode
    preprocess_input=one_hot_encode_masks,
    validation_split=0.2,
    seed=SEED,
)
print("training:")
tr_img_gen = img_data_gen.flow_from_directory(DATA_DIR, 
                                              classes=['images'],
                                              class_mode=None, 
                                              shuffle=True,
                                              subset="training")
tr_mask_gen = mask_data_gen.flow_from_directory(DATA_DIR, 
                                                classes=['masks'], 
                                                class_mode=None, 
                                                color_mode='gray',
                                                subset="training",
                                                shuffle=True)
print("validation:")
val_img_gen = img_data_gen.flow_from_directory(DATA_DIR, 
                                               classes=['images'],
                                               class_mode=None, 
                                               shuffle=True,
                                               subset="validation")
val_mask_gen = mask_data_gen.flow_from_directory(DATA_DIR, 
                                                 classes=['masks'], 
                                                 class_mode=None, 
                                                 color_mode='gray',
                                                 subset="validation",
                                                 shuffle=True)
#tr_img_gen.show_data()
#tr_mask_gen.show_data()

train_generator = zip(tr_img_gen, tr_mask_gen)
validation_generator = zip(tr_img_gen, tr_mask_gen)

# visualize images
rows = 5
image_batch, mask_batch = next(train_generator)
fix, ax = plt.subplots(rows,2, figsize=(4,rows*2))
for i, (img,mask) in enumerate(zip(image_batch, mask_batch)):
    if i>rows-1:
        break
    ax[i,0].imshow(np.uint8(img))
    ax[i,1].imshow(mask.argmax(-1))
    
plt.show()

# train the model
model.fit(
  train_generator,
  steps_per_epoch=len(train_generator),
  epochs=50,
  validation_data=validation_generator,
  validation_steps=len(validation_generator)
)

```


## Citing (BibTex):<br />
```
@misc{Tukiainen:2019,
  author = {Tukiainen, M.},
  title = {ImageDataAugmentor},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {https://github.com/mjkvaak/ImageDataAugmentor/} 
}
```

## License
This project is distributed under [MIT license](LICENSE). 
The code is heavily adapted from
https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/ (also MIT licensed)
